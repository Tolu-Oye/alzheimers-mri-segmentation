{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Hippocampus Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we propose a better framework for identifying Alzheimer's disease (AD) using U-Net variants trained on brain magnetic resonance imaging (MRI) data. We explore various state-of-the-art U-Net topologies, such as U-Net++, with the intention of enhancing the accuracy and robustness of AD diagnosis using advanced segmentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the necessary libraries that need to be installed to run the code:\n",
    "\n",
    "- OpenCV (cv2)\n",
    "- Pandas\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Torchvision\n",
    "- PyTorch\n",
    "- TQDM\n",
    "- PIL (Python Imaging Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Replace \"mps\" with \"cuda\" or the appropriate CUDA version installed on your machine as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect output: \n",
    "\n",
    "tensor([1.], device='mps:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect output:\n",
    "\n",
    "PyTorch version: 2.1.0.dev20230413\n",
    "\n",
    "Is MPS (Metal Performance Shader) built? True\n",
    "\n",
    "Is MPS available? True\n",
    "\n",
    "Using device: mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare DataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_file_paths(label_path, image_path, num):\n",
    "    '''\n",
    "    This function takes in the paths to the directories containing the label and image files, \n",
    "    as well as a number indicating whether the dataset has 35 or 100 files. \n",
    "    It then uses these paths to extract the sorted paths for the left and right hippocampus images, \n",
    "    as well as the total image paths, and returns them as lists. \n",
    "    The function also includes error handling for cases where there may be issues with the file paths.\n",
    "    '''\n",
    "    Label_Path = list(label_path.glob(r\"**/*.jpg\"))\n",
    "    Image_Path = list(image_path.glob(r\"**/*.jpg\"))\n",
    "\n",
    "    Label_Series = pd.Series(Label_Path, name=\"LABEL\", dtype='object').astype(str)\n",
    "    Image_Series = pd.Series(Image_Path, name=\"IMAGE\", dtype='object').astype(str)\n",
    "\n",
    "    if num == 35:\n",
    "        Split_Params_For_LABEL = \"35label/\"\n",
    "        Split_Params_For_IMG = \"35/\"\n",
    "    elif num == 100:\n",
    "        Split_Params_For_LABEL = \"100label/\"\n",
    "        Split_Params_For_IMG = \"100/\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for 'num'.\")\n",
    "\n",
    "    Common_Params = \"/\"\n",
    "    List_Split_Params = \"_\"\n",
    "\n",
    "    L_IMG = []\n",
    "    R_IMG = []\n",
    "    Total_IMG = []\n",
    "\n",
    "    for label_x in Label_Series:\n",
    "        try:\n",
    "            L_Main_Path, L_Target_Path = label_x.split(Split_Params_For_LABEL)\n",
    "            Label_Path_Before, Label_Path_Middle, Label_Path_After = L_Target_Path.split(Common_Params)\n",
    "            Label_Path_Split = Label_Path_After.split(List_Split_Params)\n",
    "            \n",
    "            if \"ADNI_013_S_0325_85153_ACPC\" in L_Target_Path:\n",
    "                continue\n",
    "\n",
    "            if Label_Path_Split[-2] == 'L':\n",
    "                L_IMG.append(label_x)\n",
    "            elif Label_Path_Split[-2] == 'R':\n",
    "                R_IMG.append(label_x)\n",
    "            else:\n",
    "                print(\"SOMETHING IS WRONG!\")\n",
    "        except:\n",
    "            print(\"LABEL: \", label_x)\n",
    "\n",
    "    for image_x in Image_Series:\n",
    "        L_Main_Path, L_Target_Path = image_x.split(Split_Params_For_IMG)\n",
    "        if \"ADNI_013_S_0325_85153_ACPC\" in L_Target_Path:\n",
    "                continue\n",
    "        \n",
    "        Total_IMG.append(image_x)\n",
    "\n",
    "    Sort_L = sorted(L_IMG)\n",
    "    Sort_R = sorted(R_IMG)\n",
    "    Sort_IMG = sorted(Total_IMG)\n",
    "\n",
    "    return Sort_L, Sort_R, Sort_IMG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** To use this code, please change the *folder* variable to the path where you have saved the Alzheimer's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"YOUR FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# folder = \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label100_Path = Path(folder + \"/hippocampus/label/100label/\")\n",
    "Image100_Path = Path(folder + \"/hippocampus/original/100\")\n",
    "\n",
    "Sort_L_100, Sort_R_100, Sort_IMG_100 = get_sorted_file_paths(Label100_Path, Image100_Path, 100)\n",
    "\n",
    "Label35_Path = Path(folder + \"/hippocampus/label/35label\")\n",
    "Image35_Path = Path(folder + \"/hippocampus/original/35\")\n",
    "\n",
    "Sort_L_35, Sort_R_35, Sort_IMG_35 = get_sorted_file_paths(Label35_Path, Image35_Path, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"100label - L:\", len(Sort_L_100))\n",
    "print(\"100label - R:\", len(Sort_R_100))\n",
    "print(\"100label - IMG:\", len(Sort_IMG_100))\n",
    "print(\"---------------------------------\")\n",
    "print(\"35label - L:\", len(Sort_L_35))\n",
    "print(\"35label - R:\", len(Sort_R_35))\n",
    "print(\"35label - IMG:\", len(Sort_IMG_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_prepare_data(sorted_images, sorted_left, sorted_right, threshold=250, min_pixel_count=50):\n",
    "    '''\n",
    "    This function extracts and prepares the data for training the model.\n",
    "    It loads the images and hippocampus masks, applies a threshold, and checks if there are enough pixels in the mask.\n",
    "    If the condition is met, the function adds the image and the hippocampus mask to the data to be returned.\n",
    "    '''\n",
    "    X_Image = []\n",
    "    X_Hippocampus = []\n",
    "\n",
    "    for img, left, right in zip(sorted_images, sorted_left, sorted_right):\n",
    "        left_gray = cv2.imread(left, cv2.IMREAD_GRAYSCALE)\n",
    "        right_gray = cv2.imread(right, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        left_pixel_count = np.sum(left_gray > threshold)\n",
    "        right_pixel_count = np.sum(right_gray > threshold)\n",
    "\n",
    "        if left_pixel_count > min_pixel_count or right_pixel_count > min_pixel_count:\n",
    "            image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "            left_colored = cv2.cvtColor(cv2.imread(left, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "            right_colored = cv2.cvtColor(cv2.imread(right, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "            hippocampus_concat = cv2.addWeighted(left_colored, 1, right_colored, 1, 0.2)\n",
    "            \n",
    "            X_Image.append(image)\n",
    "            X_Hippocampus.append(hippocampus_concat)\n",
    "\n",
    "    return np.array(X_Image), np.array(X_Hippocampus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Image_100, X_Hippocampus_100 = extract_and_prepare_data(Sort_IMG_100[:10000], Sort_L_100[:10000], Sort_R_100[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Image_100.shape)\n",
    "print(X_Hippocampus_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure,axis = plt.subplots(1,2,figsize=(10,10))\n",
    "\n",
    "index = 1\n",
    "\n",
    "axis[0].imshow(X_Image_100[index])\n",
    "axis[0].set_xlabel(X_Image_100[index].shape)\n",
    "axis[0].set_title(\"IMAGE\")\n",
    "\n",
    "axis[1].imshow(X_Hippocampus_100[index])\n",
    "axis[1].set_xlabel(X_Hippocampus_100[index].shape)\n",
    "axis[1].set_title(\"HIPPOCAMPUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_images(images, target_shape=256):\n",
    "    '''\n",
    "    This code defines a function to pad images to a desired target shape. \n",
    "    The function takes in an array of images and a target shape (default value is 256) \n",
    "    and returns an array of padded images. For each image, the function calculates the difference between \n",
    "    the image's shape and the target shape, and then pads the image accordingly using np.pad. \n",
    "    Finally, the function returns the padded images as a NumPy array.\n",
    "    '''\n",
    "    padded_images = []\n",
    "    for image in images:\n",
    "        pad_height = target_shape - image.shape[0]\n",
    "        pad_width = target_shape - image.shape[1]\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_image = np.pad(image, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), mode='constant')\n",
    "        padded_images.append(padded_image)\n",
    "\n",
    "    return np.array(padded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Image_100_Hippocampus = pad_images(X_Image_100)\n",
    "X_Hippocampus_100_Label = pad_images(X_Hippocampus_100)\n",
    "\n",
    "print(\"Padded 100 - ARRAY IMAGE SHAPE: \", X_Image_100_Hippocampus.shape)\n",
    "print(\"Padded 100 - ARRAY HIPPOCAMPUS SHAPE: \", X_Hippocampus_100_Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure,axis = plt.subplots(1, 2,figsize=(10,10))\n",
    "\n",
    "index = 5\n",
    "\n",
    "axis[0].imshow(X_Image_100_Hippocampus[index])\n",
    "axis[0].set_xlabel(X_Image_100_Hippocampus[index].shape)\n",
    "axis[0].set_title(\"IMAGE\")\n",
    "\n",
    "axis[1].imshow(X_Hippocampus_100_Label[index], cmap='gray')\n",
    "axis[1].set_xlabel(X_Hippocampus_100_Label[index].shape)\n",
    "axis[1].set_title(\"HIPPOCAMPUS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_masks(images, threshold=127):\n",
    "    '''\n",
    "    This function converts a set of images to binary masks using thresholding.\n",
    "    It takes as input an array of images and an optional threshold value (default is 127).\n",
    "    It returns an array of binary masks with the same shape as the input images.\n",
    "    '''\n",
    "    binary_masks = []\n",
    "    for image in images:\n",
    "        # Convert the image to grayscale if it's not already\n",
    "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray_image = image\n",
    "\n",
    "        # Apply thresholding to create a binary mask\n",
    "        _, binary_mask = cv2.threshold(gray_image,255,255,cv2.THRESH_TOZERO_INV)\n",
    "        binary_masks.append(binary_mask)\n",
    "\n",
    "    return np.array(binary_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_masks = convert_to_binary_masks(X_Hippocampus_100_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure,axis = plt.subplots(1, 2,figsize=(10,10))\n",
    "\n",
    "index = 5\n",
    "\n",
    "axis[0].imshow(X_Image_100_Hippocampus[index])\n",
    "axis[0].set_xlabel(X_Image_100_Hippocampus[index].shape)\n",
    "axis[0].set_title(\"IMAGE\")\n",
    "\n",
    "axis[1].imshow(binary_masks[index], cmap='gray')\n",
    "axis[1].set_xlabel(binary_masks[index].shape)\n",
    "axis[1].set_title(\"HIPPOCAMPUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Image_35, X_Hippocampus_35 = extract_and_prepare_data(Sort_IMG_35[:100], Sort_L_35[:100], Sort_R_35[:100])\n",
    "X_Image_35_Hippocampus = pad_images(X_Image_35)\n",
    "X_Hippocampus_35_Label = pad_images(X_Hippocampus_35)\n",
    "X_Hippocampus_35_Binary = convert_to_binary_masks(X_Hippocampus_35_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure,axis = plt.subplots(1, 2,figsize=(10,10))\n",
    "\n",
    "index = 5\n",
    "\n",
    "axis[0].imshow(X_Image_35_Hippocampus[index])\n",
    "axis[0].set_xlabel(X_Image_35_Hippocampus[index].shape)\n",
    "axis[0].set_title(\"IMAGE\")\n",
    "\n",
    "axis[1].imshow(X_Hippocampus_35_Binary[index], cmap='gray')\n",
    "axis[1].set_xlabel(X_Hippocampus_35_Binary[index].shape)\n",
    "axis[1].set_title(\"HIPPOCAMPUS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = ToTensor()\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        image_tensor = self.transform(image)\n",
    "        mask_tensor = self.transform(mask)\n",
    "\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, batch_size, split_ratio=0.8):\n",
    "    '''\n",
    "    This function takes in a dataset, batch size, and split ratio, and returns two dataloaders - \n",
    "    one for the training set and one for the validation set. \n",
    "    '''\n",
    "    trainset_len = int(len(dataset) * split_ratio)\n",
    "    valset_len = int(len(dataset) - trainset_len)\n",
    "    trainset, valset = random_split(dataset, [trainset_len, valset_len])\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def small_dataset(batch_size):\n",
    "    '''\n",
    "    This function randomly selects a subset of images and masks from the full dataset, \n",
    "    creates a CustomDataset object using these images and masks, and returns the dataloaders obtained \n",
    "    by passing this dataset to the split_dataset function.\n",
    "    '''\n",
    "    indices = torch.randperm(len(X_Image_100_Hippocampus))[:train_number]\n",
    "    images = [X_Image_100_Hippocampus[i] for i in indices]\n",
    "    masks = [binary_masks[i] for i in indices]\n",
    "\n",
    "    dataset = CustomDataset(images, masks)\n",
    "    train_loader, val_loader = split_dataset(dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def full_dataset(batch_size):\n",
    "    '''\n",
    "    This function creates a CustomDataset object using all the images and masks from the full dataset, \n",
    "    and returns the dataloaders obtained by passing this dataset to the split_dataset function.\n",
    "    '''\n",
    "    dataset = CustomDataset(X_Image_100_Hippocampus, binary_masks)\n",
    "    train_loader, val_loader = split_dataset(dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Before running this code, you need to define the value of *train_number* and *batch_size*, as they are used in the *small_dataset* function. Once you have set these values, you can call the *small_dataset* function with the desired *batch_size*, and it will return *train_loader* and *val_loader*, which are used for training and validation.\n",
    "Normalizing the images use 'Min-max normalization' or 'Standardization'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_number = 300 # choose your own value\n",
    "# batch_size = 10\n",
    "# train_loader, val_loader = small_dataset(batch_size)\n",
    "# print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# train_loader, val_loader = full_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expext output:\n",
    "\n",
    "(torch.Size([10, 3, 256, 256]), torch.Size([10, 1, 256, 256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two index lists for train and validation data splitting. The total number of images used for training is specified by the variable train_number. The code first creates a list of indices with the length of train_number. It then splits the list into two parts using a 0.2 validation split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(train_number))\n",
    "split = int(np.floor(0.2 * train_number))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loader\n",
    "test_dataset = CustomDataset(X_Image_35_Hippocampus, X_Hippocampus_35_Binary)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net/U-Net++ models from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Create a U-Net model\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name=\"resnet34\", # choose encoder, e.g. resnet34, resnet50, etc.\n",
    "    encoder_weights=\"imagenet\", # use pre-trained weights for encoder initialization\n",
    "    in_channels=3, # number of input channels\n",
    "    classes=1, # number of output channels\n",
    ")\n",
    "\n",
    "# Create a U-Net++ model\n",
    "unet_plusplus_model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\", # choose encoder, e.g. resnet34, resnet50, etc.\n",
    "    encoder_weights=\"imagenet\", # use pre-trained weights for encoder initialization\n",
    "    in_channels=3, # number of input channels\n",
    "    classes=1, # number of output channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from segmentation_models_pytorch.utils import metrics\n",
    "\n",
    "model = unet_plusplus_model.to(device)  \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Instantiate the Dice coefficient metric\n",
    "dice_metric = metrics.Fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch:\", epoch)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_dice_score = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch loss and dice score\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_dice_score += dice_metric(outputs, targets).item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss and dice score for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "    epoch_dice_score /= num_batches\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Loss = {epoch_loss:.4f}, Dice Score = {epoch_dice_score:.4f}\")\n",
    "\n",
    "    # Save model checkpoint (optional)\n",
    "    # torch.save(model.state_dict(), f\"unet_checkpoint_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # Validation loop (optional)\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     ... # Perform validation using the same approach as the training loop\n",
    "    #     # Print validation statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net/U-Net++ model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        '''\n",
    "        TODO: choose your own 'base_channels' value\n",
    "        '''\n",
    "        base_channels = 32\n",
    "#         base_channels = 64\n",
    "        filters = [base_channels * i for i in [1, 2, 4, 8, 16]]\n",
    "\n",
    "        # Encoder part\n",
    "        self.encoders = nn.ModuleList([\n",
    "            ConvBlock(in_channels, filters[0]),\n",
    "            ConvBlock(filters[0], filters[1]),\n",
    "            ConvBlock(filters[1], filters[2]),\n",
    "            ConvBlock(filters[2], filters[3]),\n",
    "            ConvBlock(filters[3], filters[4]),\n",
    "        ])\n",
    "\n",
    "        self.pools = nn.ModuleList([nn.MaxPool2d(kernel_size=2, stride=2) for _ in range(4)])\n",
    "\n",
    "        # Decoder part\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            UpConv(filters[4], filters[3]),\n",
    "            UpConv(filters[3], filters[2]),\n",
    "            UpConv(filters[2], filters[1]),\n",
    "            UpConv(filters[1], filters[0]),\n",
    "        ])\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            ConvBlock(filters[4], filters[3]),\n",
    "            ConvBlock(filters[3], filters[2]),\n",
    "            ConvBlock(filters[2], filters[1]),\n",
    "            ConvBlock(filters[1], filters[0]),\n",
    "        ])\n",
    "\n",
    "        self.output_conv = nn.Conv2d(filters[0], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # Use dropout\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        encoder_outputs = []\n",
    "        for i in range(5):\n",
    "            x = self.encoders[i](x)\n",
    "            encoder_outputs.append(x)\n",
    "            if i < 4:\n",
    "                x = self.pools[i](x)\n",
    "\n",
    "        # Decoder part\n",
    "        for i in range(4):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((encoder_outputs[-(i + 2)], x), dim=1)\n",
    "            x = self.decoders[i](x)\n",
    "\n",
    "        out = self.output_conv(x)\n",
    "        \n",
    "        # Use a Sigmoid activation function for final output layer \n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Architecture\n",
    "\n",
    "UNet(\n",
    "  (encoders): ModuleList(\n",
    "    (0): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (1): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (2): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (3): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (4): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pools): ModuleList(\n",
    "    (0-3): 4 x MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (upconvs): ModuleList(\n",
    "    (0): UpConv(\n",
    "      (up): Sequential(\n",
    "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
    "        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (3): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (1): UpConv(\n",
    "      (up): Sequential(\n",
    "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
    "        (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (3): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (2): UpConv(\n",
    "      (up): Sequential(\n",
    "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
    "        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (3): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (3): UpConv(\n",
    "      (up): Sequential(\n",
    "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
    "        (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (3): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (decoders): ModuleList(\n",
    "    (0): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (1): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (2): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (3): ConvBlock(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (5): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (output_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, dropout_rate=0.5):\n",
    "        super(NestedConvBlock, self).__init__()\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.match_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        identity = self.match_channels(identity)\n",
    "        \n",
    "        x = x + identity\n",
    "        output = self.activation(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, deep_supervision=True, dropout_rate=0.5):\n",
    "        super(UNetPlusPlus, self).__init__()\n",
    "\n",
    "        '''\n",
    "        TODO: choose your own 'base_channels' value\n",
    "        '''\n",
    "        base_channels = 32\n",
    "#         base_channels = 64\n",
    "        filters = [base_channels * i for i in [1, 2, 4, 8, 16]]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        #  Define the convolutional layers for each level of the UNet++\n",
    "        self.conv0_0 = NestedConvBlock(in_channels, filters[0], filters[0], dropout_rate)\n",
    "        self.conv1_0 = NestedConvBlock(filters[0], filters[1], filters[1], dropout_rate)\n",
    "        self.conv2_0 = NestedConvBlock(filters[1], filters[2], filters[2], dropout_rate)\n",
    "        self.conv3_0 = NestedConvBlock(filters[2], filters[3], filters[3], dropout_rate)\n",
    "        self.conv4_0 = NestedConvBlock(filters[3], filters[4], filters[4], dropout_rate)\n",
    "\n",
    "        self.conv0_1 = NestedConvBlock(filters[0] + filters[1], filters[0], filters[0], dropout_rate)\n",
    "        self.conv1_1 = NestedConvBlock(filters[1] + filters[2], filters[1], filters[1], dropout_rate)\n",
    "        self.conv2_1 = NestedConvBlock(filters[2] + filters[3], filters[2], filters[2], dropout_rate)\n",
    "        self.conv3_1 = NestedConvBlock(filters[3] + filters[4], filters[3], filters[3], dropout_rate)\n",
    "\n",
    "        self.conv0_2 = NestedConvBlock(filters[0]*2 + filters[1], filters[0], filters[0], dropout_rate)\n",
    "        self.conv1_2 = NestedConvBlock(filters[1]*2 + filters[2], filters[1], filters[1], dropout_rate)\n",
    "        self.conv2_2 = NestedConvBlock(filters[2]*2 + filters[3], filters[2], filters[2], dropout_rate)\n",
    "\n",
    "        self.conv0_3 = NestedConvBlock(filters[0]*3 + filters[1], filters[0], filters[0], dropout_rate)\n",
    "        self.conv1_3 = NestedConvBlock(filters[1]*3 + filters[2], filters[1], filters[1], dropout_rate)\n",
    "\n",
    "        self.conv0_4 = NestedConvBlock(filters[0]*4 + filters[1], filters[0], filters[0], dropout_rate)\n",
    "\n",
    "        self.final = nn.Conv2d(filters[0], out_channels, kernel_size=1) \n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            self.final1 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            output1 = self.final1(x0_1)\n",
    "            output1 = torch.sigmoid(output1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output2 = torch.sigmoid(output2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output3 = torch.sigmoid(output3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            output4 = torch.sigmoid(output4)\n",
    "            return [output1, output2, output3, output4]\n",
    "\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            output = torch.sigmoid(out)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Architecture\n",
    "\n",
    "UNetPlusPlus(\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (Up): Upsample(scale_factor=2.0, mode='bilinear')\n",
    "  (conv0_0): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv1_0): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv2_0): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv3_0): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv4_0): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv0_1): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv1_1): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv2_1): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv3_1): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv0_2): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv1_2): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv2_2): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv0_3): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(160, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv1_3): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (conv0_4): NestedConvBlock(\n",
    "    (activation): ReLU(inplace=True)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (conv1): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (match_channels): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  )\n",
    "  (final): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "  (final1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "  (final2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "  (final3): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "  (final4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Choose model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = UNet().to(device)\n",
    "# model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = UNetPlusPlus(in_channels=3, out_channels=1, deep_supervision=True, dropout_rate=0.5).to(device)\n",
    "# model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(prediction, target):\n",
    "    '''\n",
    "    The Dice loss is a measure of the overlap between the prediction and target.\n",
    "    It ranges between 0 and 1, with 1 indicating a perfect match (lower is better).\n",
    "    '''\n",
    "    smooth = 1.0\n",
    "    i_flat = prediction.view(-1)\n",
    "    t_flat = target.view(-1)\n",
    "    intersection = (i_flat * t_flat).sum()\n",
    "    return 1 - ((2. * intersection + smooth) / (i_flat.sum() + t_flat.sum() + smooth))\n",
    "\n",
    "def calc_loss(prediction, target, bce_weight=0.5):\n",
    "    '''\n",
    "    This function calculates a combined loss using binary cross entropy (BCE) and Dice loss.\n",
    "    The bce_weight determines the contribution of BCE to the final loss value.\n",
    "    '''\n",
    "    bce = F.binary_cross_entropy_with_logits(prediction, target)\n",
    "    prediction = F.sigmoid(prediction)\n",
    "    dice = dice_loss(prediction, target)\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    return loss\n",
    "\n",
    "def combined_loss(y_pred, y_true, alpha=0.5, beta=0.5):\n",
    "    '''\n",
    "    This function calculates a combined loss using binary cross entropy (BCE) and the complement of the Dice coefficient.\n",
    "    The alpha and beta parameters determine the contributions of BCE loss and the complement of the \n",
    "    Dice coefficient to the final loss value.\n",
    "    '''\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(y_pred, y_true)\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    dice_coeff = 1 - dice_loss(y_pred, y_true)\n",
    "    return alpha * bce_loss + beta * dice_coeff\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(output, target):\n",
    "    '''\n",
    "    Calculates the Intersection over Union (IoU) score between the predicted output and the target. \n",
    "    The IoU is a metric used to evaluate the accuracy of image segmentation models. \n",
    "    It is calculated as the ratio of the intersection of the predicted output and the target to \n",
    "    the union of the predicted output and the target.\n",
    "    '''\n",
    "    smooth = 1e-5\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "def dice_coeff(im1, im2, empty_score=1.0):\n",
    "    '''\n",
    "    Calculates the Dice coefficient between two images im1 and im2. \n",
    "    The Dice coefficient is a similarity metric used to compare the pixel-wise agreement between two binary images.\n",
    "    It is calculated as twice the intersection between the two images divided by the sum of pixels in both images.\n",
    "    '''\n",
    "    im1 = (im1 > 0.5).astype(bool)\n",
    "    im2 = (im2 > 0.5).astype(bool)\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Error!\")\n",
    "    intersection = np.logical_and(im1, im2).sum()\n",
    "    im_sum = im1.sum() + im2.sum()\n",
    "    if im_sum == 0:\n",
    "        return empty_score\n",
    "    return 2.0 * intersection / im_sum\n",
    "\n",
    "\n",
    "def numeric_scores(output, target):\n",
    "    '''\n",
    "    Calculates the false positive (FP), false negative (FN), true positive (TP), and true negative (TN) scores \n",
    "    between the predicted output and the ground truth. These scores are used to evaluate the performance of binary \n",
    "    classifiers.\n",
    "    '''\n",
    "    output = (output > 0.5).astype(bool)\n",
    "    target = (target > 0.5).astype(bool)\n",
    "    FP = np.sum((output == 1) & (target == 0))\n",
    "    FN = np.sum((output == 0) & (target == 1))\n",
    "    TP = np.sum((output == 1) & (target == 1))\n",
    "    TN = np.sum((output == 0) & (target == 0))\n",
    "\n",
    "    return FP, FN, TP, TN\n",
    "\n",
    "\n",
    "def accuracy_score(output, target):\n",
    "    '''\n",
    "    Calculates the accuracy score between the predicted output and the ground truth. \n",
    "    The accuracy score is a metric used to evaluate the overall performance of binary classifiers. \n",
    "    It is calculated as the ratio of the number of correct predictions to the total number of predictions made.\n",
    "    '''\n",
    "    FP, FN, TP, TN = numeric_scores(output, target)\n",
    "    N = FP + FN + TP + TN\n",
    "    accuracy = (TP + TN) / N\n",
    "    return accuracy * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Try different optimizers and schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model_test.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.SGD(model_test.parameters(), lr=0.0001, momentum=0.99)\n",
    "# optimizer = torch.optim.Adam(model_test.parameters(), lr=3e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, int(1e10), eta_min=1e-5)\n",
    "# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-2, step_size_up=2000, mode='triangular2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "\n",
    "'''\n",
    "TODO: choose 'initialize_weights' or not\n",
    "'''\n",
    "# initialize_weights(model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "deep_supervision = isinstance(model_test, UNetPlusPlus)\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch {i+1}/{epoch}\")\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model_test.train()\n",
    "\n",
    "    for batch_idx, (train, ground_truth) in enumerate(train_loader):\n",
    "        train, ground_truth = train.to(device), ground_truth.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_predect = model_test(train)\n",
    "        loss_train_batch = 0\n",
    "        \n",
    "        if deep_supervision:\n",
    "            for output in train_predect:\n",
    "                '''\n",
    "                TODO: choose loss function\n",
    "                '''\n",
    "#                 loss_train_batch = dice_loss(output, ground_truth)\n",
    "#                 loss_train_batch = calc_loss(output, ground_truth)    \n",
    "#                 loss_train_batch = combined_loss(output, ground_truth)  \n",
    "#                 loss_train_batch = criterion(output, ground_truth)\n",
    "            loss_train_batch /= len(train_predect)\n",
    "        else:\n",
    "#             loss_train_batch = dice_loss(train_predect, ground_truth)\n",
    "#             loss_train_batch = calc_loss(train_predect, ground_truth)    \n",
    "#             loss_train_batch = combined_loss(train_predect, ground_truth)  \n",
    "#             loss_train_batch = criterion(train_predect, ground_truth)\n",
    "    \n",
    "        train_loss += loss_train_batch.item() * train.size(0)\n",
    "        loss_train_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Batch {batch_idx + 1} loss: {loss_train_batch.item():.4f}\")\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    model_test.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val, ground_truth in val_loader:\n",
    "            val, ground_truth = val.to(device), ground_truth.to(device)\n",
    "\n",
    "            val_predect = model_test(val)\n",
    "            loss_val_batch = 0\n",
    "\n",
    "            if deep_supervision:\n",
    "                for output in val_predect:\n",
    "                    '''\n",
    "                    TODO: choose loss function\n",
    "                    '''\n",
    "#                     loss_val_batch = dice_loss(output, ground_truth)\n",
    "#                     loss_val_batch = calc_loss(output, ground_truth)     \n",
    "#                     loss_val_batch = combined_loss(output, ground_truth) \n",
    "#                     loss_val_batch = criterion(output, ground_truth)\n",
    "                loss_val_batch /= len(val_predect)\n",
    "            else:\n",
    "#                 loss_val_batch = dice_loss(val_predect, ground_truth)\n",
    "#                 loss_val_batch = calc_loss(val_predect, ground_truth)    \n",
    "#                 loss_val_batch = combined_loss(val_predect, ground_truth) \n",
    "#                 loss_val_batch = criterion(val_predect, ground_truth)\n",
    "\n",
    "            valid_loss += loss_val_batch.item() * val.size(0)\n",
    "        \n",
    "\n",
    "    train_loss = train_loss / len(train_idx)\n",
    "    valid_loss = valid_loss / len(valid_idx)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(i + 1, epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "'''\n",
    "TODO: choose image title\n",
    "'''\n",
    "# plt.title('U-Net')\n",
    "# plt.title('U-Net++')\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the output tensor w/ threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.eval()\n",
    "\n",
    "image, ground_truth = next(iter(train_loader))\n",
    "image = image.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_test(image)\n",
    "\n",
    "outputs = outputs.cpu()\n",
    "\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "binary_mask = (outputs > threshold).float()\n",
    "print(binary_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: choose evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate U-Net\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            for i in range(outputs.shape[0]):\n",
    "                output = outputs[i].cpu().numpy()\n",
    "                target = targets[i].cpu().numpy()\n",
    "\n",
    "                iou_scores.append(iou_score(output, target))\n",
    "                dice_scores.append(dice_coeff(output, target))\n",
    "                accuracy_scores.append(accuracy_score(output, target))\n",
    "\n",
    "    iou_avg = np.mean(iou_scores)\n",
    "    dice_avg = np.mean(dice_scores)\n",
    "    accuracy_avg = np.mean(accuracy_scores)\n",
    "\n",
    "    return iou_avg, dice_avg, accuracy_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate U-Net++\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            output = outputs[-1]\n",
    "            output = torch.sigmoid(output)\n",
    "\n",
    "            for i in range(output.shape[0]):\n",
    "                output_np = output[i].cpu().numpy()\n",
    "                target_np = targets[i].cpu().numpy()\n",
    "\n",
    "                iou_scores.append(iou_score(output_np, target_np))\n",
    "                dice_scores.append(dice_coeff(output_np, target_np))\n",
    "                accuracy_scores.append(accuracy_score(output_np, target_np))\n",
    "\n",
    "    iou_avg = np.mean(iou_scores)\n",
    "    dice_avg = np.mean(dice_scores)\n",
    "    accuracy_avg = np.mean(accuracy_scores)\n",
    "\n",
    "    return iou_avg, dice_avg, accuracy_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_avg, dice_avg, accuracy_avg = evaluate_model(model_test, test_loader)\n",
    "\n",
    "print(f\"IoU Score: {iou_avg:.4f}\")\n",
    "print(f\"Dice Coefficient: {dice_avg:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_avg:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: choose preoricess image with deep supervision or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_np, input_shape):\n",
    "    image = Image.fromarray(image_np)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_shape[1:]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    output_np = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output_np = (output_np > 0.5).astype(np.uint8) * 255\n",
    "    return output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "input_shape = (3, 256, 256)\n",
    "preprocessed_image = preprocess_image(X_Image_100_Hippocampus[idx], input_shape).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(preprocessed_image)\n",
    "\n",
    "predicted_segmentation = postprocess_output(output)\n",
    "\n",
    "figure, axis = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "axis[0].imshow(X_Image_100_Hippocampus[idx], cmap='gray')\n",
    "axis[0].set_title('Original Test Image')\n",
    "\n",
    "axis[1].imshow(predicted_segmentation, cmap='gray')\n",
    "axis[1].set_title('Predicted Segmentation')\n",
    "\n",
    "axis[2].imshow(X_Hippocampus_100_Label[idx], cmap='gray')\n",
    "axis[2].set_title('Ground Truth Mask')\n",
    "\n",
    "for ax in axis:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "binary_mask = (output > threshold).float()\n",
    "print(binary_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_np, input_shape):\n",
    "    image = Image.fromarray(image_np)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_shape[1:]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "def postprocess_output(output_list):\n",
    "    output_tensor = output_list[-1]\n",
    "    output_np = output_tensor.squeeze().cpu().detach().numpy()\n",
    "    output_np = (output_np > 0.5).astype(np.uint8) * 255\n",
    "    return output_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "input_shape = (3, 256, 256)\n",
    "preprocessed_image = preprocess_image(X_Image_100_Hippocampus[idx], input_shape).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_test.eval()\n",
    "    output = model_test(preprocessed_image)\n",
    "\n",
    "predicted_segmentation = postprocess_output(output)\n",
    "\n",
    "figure, axis = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "axis[0].imshow(X_Image_100_Hippocampus[idx], cmap='gray')\n",
    "axis[0].set_title('Original Test Image')\n",
    "\n",
    "axis[1].imshow(predicted_segmentation, cmap='gray')\n",
    "axis[1].set_title('Predicted Segmentation')\n",
    "\n",
    "axis[2].imshow(X_Hippocampus_100_Label[idx], cmap='gray')\n",
    "axis[2].set_title('Ground Truth Mask')\n",
    "\n",
    "for ax in axis:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "binary_mask = (output[-1] > threshold).float()\n",
    "print(binary_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addtional: U-Net with ResNeXt50 backbone.Unet with ResNeXt50 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from torchvision.models import resnext50_32x4d\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convrelu = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convrelu(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvRelu(in_channels, in_channels // 4, 1, 0)\n",
    "        \n",
    "        self.deconv = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "        \n",
    "        self.conv2 = ConvRelu(in_channels // 4, out_channels, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.deconv(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class ResNeXtUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = resnext50_32x4d(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        filters = [4*64, 4*128, 4*256, 4*512]\n",
    "        \n",
    "        # Down\n",
    "        self.encoder0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.encoder1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.encoder2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.encoder3 = nn.Sequential(*self.base_layers[6])\n",
    "        self.encoder4 = nn.Sequential(*self.base_layers[7])\n",
    "\n",
    "        # Up\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        # Final Classifier\n",
    "        self.last_conv0 = ConvRelu(256, 128, 3, 1)\n",
    "        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)\n",
    "                       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Down\n",
    "        x = self.encoder0(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Up + sc\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "        #print(d1.shape)\n",
    "\n",
    "        # final classifier\n",
    "        out = self.last_conv0(d1)\n",
    "        out = self.last_conv1(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(model, loader, threshold=0.3):\n",
    "    valloss = 0\n",
    "    with torch.no_grad():\n",
    "        for i_step, (data, target) in enumerate(loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
    "\n",
    "            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            valloss += picloss\n",
    "\n",
    "    return valloss / i_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx50 = ResNeXtUNet(n_classes=1).to(device)\n",
    "output = rx50(torch.randn(1,3,256,256).to(device))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(inputs, target):\n",
    "    smooth = 1.0\n",
    "    intersection = 2.0 * ((target * inputs).sum()) + smooth\n",
    "    union = target.sum() + inputs.sum() + smooth\n",
    "    return 1 - (intersection / union)\n",
    "\n",
    "\n",
    "def bce_dice_loss(inputs, target):\n",
    "    dicescore = dice_coef_loss(inputs, target)\n",
    "    bcescore = nn.BCELoss()\n",
    "    bceloss = bcescore(inputs, target)\n",
    "    return bceloss + dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_metric(inputs, target):\n",
    "    intersection = 2.0 * (target * inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):  \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        losses = []\n",
    "        train_iou = []\n",
    "                \n",
    "        if lr_scheduler:\n",
    "            warmup_factor = 1.0 / 100\n",
    "            warmup_iters = min(100, len(train_loader) - 1)\n",
    "            lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "        \n",
    "        \n",
    "        for i_step, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "                      \n",
    "            outputs = model(data)\n",
    "            \n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            \n",
    "            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            \n",
    "            loss = train_loss(outputs, target)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            train_iou.append(train_dice)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "        val_mean_iou = compute_iou(model, val_loader)\n",
    "        \n",
    "        loss_history.append(np.array(losses).mean())\n",
    "        train_history.append(np.array(train_iou).mean())\n",
    "        val_history.append(val_mean_iou)\n",
    "        \n",
    "        print(\"Epoch [%d]\" % (epoch))\n",
    "        print(\"Mean loss on train:\", np.array(losses).mean(), \n",
    "              \"\\nMean DICE on train:\", np.array(train_iou).mean(),\n",
    "              \"\\nMean DICE on validation:\", val_mean_iou)\n",
    "        \n",
    "    return loss_history, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx50_optimizer = torch.optim.Adam(rx50.parameters(), lr=5e-4)\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx50_lh, rx50_th, rx50_vh = train_model(\"ResNeXt50\", rx50, train_loader, val_loader, bce_dice_loss, rx50_optimizer, False, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iou = compute_iou(rx50, test_loader)\n",
    "print(f\"\"\"ResNext50\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: Kaggle, MRI Hippocampus Segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
